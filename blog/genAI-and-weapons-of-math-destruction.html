<!DOCTYPE html><html lang="en"><head><meta charset="UTF-8"><link rel="icon" type="image/svg+xml" href="/favicon.svg"><meta name="viewport" content="width=device-width,initial-scale=1"><title>Generative AI, A Weapon of Math Destruction in the Making?</title><script type="module" crossorigin="" src="/assets/app-BU0oFdFQ.js"></script><link rel="stylesheet" crossorigin="" href="/assets/app-DSsrvWcW.css"><link rel="modulepreload" crossorigin="" href="/assets/genAI-and-weapons-of-math-destruction-Da7qRx_F.js"><meta property="og:title" content="Generative AI, A Weapon of Math Destruction in the Making?"><meta name="twitter:title" content="Generative AI, A Weapon of Math Destruction in the Making?"><meta name="description" content="After reading Cathy O‚ÄôNeil‚Äôs brilliant *Weapons of Math Destruction,* I found myself thinking deeply about how her insights apply to the explosive rise of Generative AI..."><meta property="og:description" content="After reading Cathy O‚ÄôNeil‚Äôs brilliant *Weapons of Math Destruction,* I found myself thinking deeply about how her insights apply to the explosive rise of Generative AI..."><meta name="twitter:description" content="After reading Cathy O‚ÄôNeil‚Äôs brilliant *Weapons of Math Destruction,* I found myself thinking deeply about how her insights apply to the explosive rise of Generative AI..."></head><body><div id="app" data-server-rendered="true"><div class="grid md:grid-cols-4 h-screen w-screen place-items-center" data-v-87213ebe=""><div class="md:col-span-1 w-full pl-4 md:pl-16 md:h-screen h-auto md:relative fixed navbar-background md:bg-transparent" data-v-87213ebe=""><div class="fixed md:absolute top-0 right-0 left-0 p-4 z-50 shadow-md md:shadow-none navbar-background md:bg-transparent" data-v-87213ebe=""><p class="text-xl" data-v-87213ebe="">Gabriel Vitali<span class="animate-ping" data-v-87213ebe="">_</span></p><button class="md:hidden right-0 top-0 p-4 absolute z-50" data-v-87213ebe=""><svg class="w-6 h-6" fill="none" stroke="currentColor" viewBox="0 0 24 24" xmlns="http://www.w3.org/2000/svg" data-v-87213ebe=""><path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M4 6h16M4 12h16M4 18h16" data-v-87213ebe=""></path></svg></button></div><div class="hidden md:block" data-v-87213ebe=""><div class="md:block" data-v-87213ebe=""><!--[--><nav class="h-screen w-1/2 grid content-center space-y-10"><a href="/" class="text-2xl cursor-pointer">Home</a><a href="/resume" class="text-2xl cursor-pointer">Resume</a><a href="/blog" class="router-link-active text-2xl cursor-pointer">Blog</a><a href="/portfolio" class="text-2xl cursor-pointer">Portfolio</a></nav><div class="absolute bottom-0 left-0 p-5 w-[90%] bg-gray-900 rounded-tr-2xl"><p class="mb-4">Let's connect! üöÄ</p><div class="flex flex-start flex-wrap gap-2"><a class="inline-flex items-center gap-2 bg-gray-800 py-1 px-2 rounded border" href="https://github.com/gabcvit" target="_blank"><img class="w-6" alt="GitHub tag icon" src="https://cdn.simpleicons.org/github/dddddd"><span class="text-xs font-medium text-white">GitHub</span></a><a class="inline-flex items-center gap-2 bg-gray-800 py-1 px-2 rounded border" href="https://stackoverflow.com/users/6231562/gabcvit" target="_blank"><img class="w-6" alt="Stackoverflow tag icon" src="https://cdn.simpleicons.org/stackoverflow"><span class="text-xs font-medium text-white">Stackoverflow</span></a><a class="inline-flex items-center gap-2 bg-gray-800 py-1 px-2 rounded border" href="https://www.linkedin.com/in/gabcvit/" target="_blank"><img class="w-6" alt="LinkedIn tag icon" src="https://cdn.jsdelivr.net/gh/devicons/devicon@latest/icons/linkedin/linkedin-original.svg"><span class="text-xs font-medium text-white">LinkedIn</span></a><a class="inline-flex items-center gap-2 bg-gray-800 py-1 px-2 rounded border" href="https://bsky.app/profile/gabcvit.bsky.social" target="_blank"><img class="w-6" alt="BlueSky tag icon" src="https://cdn.simpleicons.org/bluesky"><span class="text-xs font-medium text-white">BlueSky</span></a><a class="inline-flex items-center gap-2 bg-gray-800 py-1 px-2 rounded border" href="mailto:vitali.gabriel@gmail.com" target="_blank"><img class="w-6" alt="Email tag icon" src="https://cdn.simpleicons.org/mailboxdotorg/gray"><span class="text-xs font-medium text-white">Email</span></a><a class="inline-flex items-center gap-2 bg-gray-800 py-1 px-2 rounded border" href="https://gabcvit.github.io/feed.xml" target="_blank"><img class="w-6" alt="RSS tag icon" src="https://cdn.simpleicons.org/rss"><span class="text-xs font-medium text-white">RSS</span></a></div></div><!--]--></div></div></div><div class="md:col-span-3 w-full h-screen md:overflow-y-scroll px-4 md:pl-0 md:pr-16 pb-16 pt-8 md:pt-0" data-v-87213ebe=""><div class="blog-post" data-v-87213ebe=""><h1>Generative AI: A Weapon of Math Destruction in the Making?</h1><p>After reading Cathy O‚ÄôNeil‚Äôs brilliant <em>Weapons of Math Destruction,</em> I found myself thinking about how would her insights in algorthm and big data apply to the rise of Generative AI (GenAI). While GenAI holds big promises to revolutionize industries, it also risks replicating many of the same systemic issues highlighted in her analysis.</p><p>Cathy warned in her book about the dangers of <strong>opaque, unfair, and unaccountable systems</strong>‚Äîalgorithms that perpetuate biases and create harmful feedback loops. Today‚Äôs GenAI systems, despite their transformative capabilities, could easily fall into these same traps if developers do not have these concerns on their radar.</p><h2>Key Risks of GenAI Through Cathy‚Äôs Lens</h2><h3>1. <strong>Opacity</strong></h3><p>GenAI systems, like large language models, are often described as ‚Äúblack boxes.‚Äù Their outputs, regardless if text, images, or code, are the result of incredibly complex processes that are extremely difficult, if not impossible, to fully explain or completely understand the chain of thought / criteria used for the conclusion.</p><ul><li><strong>The Problem</strong>: If even developers cannot explain <em>why</em> a GenAI system produced a specific result, how can users trust its‚Äô fairness or reliability?</li><li><strong>The Risk</strong>: This lack of transparency can erode public trust, making it impossible to hold systems accountable for harmful or biased outcomes.</li></ul><p>And this is already happening through the extremely rampant adoption of GenAI models in the market withotu the proper scrutiny, for instance when allowing it to read job applications and share outcomes without in an intransparent manner (similarly to the books‚Äô case of Apple‚Äôs CV-reading algorithm which was identified to enforce sexist biases).</p><h3>2. <strong>Amplification of Bias</strong></h3><p>GenAI learns from massive datasets scraped from the internet‚Äîdata that reflects the biases, stereotypes, and inequities of our world.</p><ul><li><strong>The Problem</strong>: ‚ÄúBiased data in, biased outcomes out.‚Äù GenAI doesn‚Äôt just reflect societal biases; it can amplify them by presenting them as neutral or factual.</li><li><strong>The Risk</strong>: Outputs that reinforce harmful stereotypes or exclude marginalized groups can create real-world harm, especially when used in sensitive applications like hiring, education, or healthcare.</li></ul><h3>3. <strong>Scale and Power Concentration</strong></h3><p>GenAI is being deployed at unprecedented scale across industries, from customer service to content creation to medicine. However, much of this power is concentrated in the hands of a few large tech companies.</p><ul><li><strong>The Problem</strong>: These companies control not only the development of GenAI but also its deployment, creating significant imbalances in who benefits and who bears the risks.</li><li><strong>The Risk</strong>: Without proper oversight, these systems could exacerbate inequality, prioritizing profit over societal good.</li></ul><h3>4. <strong>Feedback Loops of Harm</strong></h3><p>When biased AI systems are used to make decisions, they can create self-reinforcing cycles of harm without proper human intervention.</p><ul><li><strong>Example</strong>: Imagine a GenAI-based hiring tool that favors candidates from certain universities. Over time, this could exclude talented individuals from nontraditional backgrounds, entrenching inequality in the workforce.</li></ul><h2>Building Solutions: How to apply what we have seen in this book</h2><p>To avoid turning GenAI into a ‚ÄúWeapon of Math Destruction,‚Äù we must take proactive steps to address these risks. Cathy‚Äôs solutions for algorithmic systems are just as relevant today:</p><h3>1. <strong>Transparency</strong></h3><ul><li>Require AI companies to disclose how models are trained, the data sources used, and the biases identified during development.</li><li>Encourage the development of ‚Äúexplainable AI‚Äù tools that allow users to understand how GenAI arrives at its conclusions.</li></ul><h3>2. <strong>Accountability</strong></h3><ul><li>Mandate regulations holding organizations responsible for the outputs of their AI systems. The result of an GenAI model cannot be seen as absolute truth or neutral.</li><li>Develop industry standards for ethical AI practices, similar to financial or environmental audits.</li></ul><h3>3. <strong>Fairness Audits</strong></h3><ul><li>Implement regular audits to evaluate the societal impact of GenAI systems, focusing on how they affect marginalized groups.</li><li>Include diverse stakeholders in the auditing process to ensure broad representation and fairness. Making sure that the data we feed our models do not echo our own bubble.</li></ul><h3>4. <strong>Empowered Oversight</strong></h3><ul><li>Create independent watchdog organizations to monitor the deployment of GenAI systems and investigate potential harms.</li><li>Establish ethical review boards within companies to assess AI projects before deployment.</li></ul><h2>The Road Ahead</h2><p>As someone deeply passionate about reducing bias and promoting fairness in technology, I believe these solutions aren‚Äôt just idealistic. <strong>They‚Äôre necessary.</strong></p><p>The stakes are too high to ignore. Generative AI is shaping how we work, communicate, and make decisions. If we don‚Äôt address its risks now, we risk entrenching inequality on a massive scale.</p><p>Cathy O‚ÄôNeil‚Äôs work reminds us that algorithms are not inherently fair, they reflect the values of those who create them. Let‚Äôs ensure that as we build the future of AI, we prioritize transparency, fairness, and accountability.</p></div></div></div></div></body></html>